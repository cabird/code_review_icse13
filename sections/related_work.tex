%!TEX root = paper.tex

%%%%%%%%%%%%%%%%%%%%%%
\section{Related Work} \label{sec:related_work}
%%%%%%%%%%%%%%%%%%%%%%


Previous studies exist that have examined the practices of code inspection and
code review.  Stein et al. conducted a study focusing specifically on
distributed, asynchronous code inspections [17]. The study included evaluation
of a tool that allowed for identification and sharing of code faults or
defects. Participants at separated locations can then discuss faults via the
tool. Laitenburger conducted a survey of code inspection methods, and presented
a taxonomy of code inspection techniques (Laitenberger 2002). Johnson conducted
an investigation into code review practices in open source development and the
effect they have on choices made by software project managers (Johnson 2006).

Porter et al. (Porter, Siy and Votta 1996) reported on a review of studies on
code inspection in 1995 that examined the effects of factors such as team size,
type of review, and number of sessions on code inspections.  They also assessed
costs and benefits across a number of studies.  These studies differ from ours
in that they were not tool-based and were the majority involved planned
meetings to discuss the code.

However, prior research also sheds light on why review today is more often
tool-based, informal, and often asynchronous. The current state of code review
might be due to the time required for more formal inspections.  Votta found
that 20\% of the interval in a ``traditional inspection'' is wasted due to
scheduling (Votta 1993). The ICICLE tool (Brothers, Sembugamoorthy and Muller
1990), or ``Intelligent Code Inspection in a C Language Environment,'' was
developed after researchers at Bellcore observed how much time and work was
expended before and during formal code inspections. Many of today√≠s review
tools are based on ideas that originated in ICICLE.  Other similar tools have
been developed in an effort to reduce time for inspection and allow
asynchronous work on reviews.   Examples include CAIS  (Mashayekhi, Feulner and
Riedl 1994) and Scrutiny (Gintell, et al. 1993).

More recently, Rigby has done extensive work examining code review practices in
open source software development (Rigby, Cleary, et al. 2012).  For example in
a study of practices in the Apache project (Rigby, German and Storey, Open
source software peer review practices: a case study of the apache server 2008)
they data-mined the email archives and found that reviews were typically small
and frequent, and that the contributions to a review were often brief and
independent from one another.

Sutherland and Venolia conducted a study at Microsoft regarding using code
review data for later information needs (Sutherland and Venolia 2009). They
hypothesized that the knowledge exchanged during code reviews could be of great
value to engineers later trying to understand or modify the discussed code.
They found that ``the meat of the code review dialog, no matter what medium, is
the articulation of design rationale'' and, thus, ``code reviews are an enticing
opportunity for capturing design rationale.''

When studying developer work habits, Latoza et al. found that many problems
encountered by developers were related to understanding the rationale behind
code changes and gathering knowledge from other members of their team (LaToza,
Venolia and DeLine 2006).
